{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "common-liability",
   "metadata": {
    "papermill": {
     "duration": 0.024017,
     "end_time": "2021-06-07T02:08:30.310995",
     "exception": false,
     "start_time": "2021-06-07T02:08:30.286978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 반드시 처음부터 끝까지 스켈레톤 코드를 살펴보고 구현하기 시작하길 바란다\n",
    "\n",
    "## 1. 스켈레톤 코드를 [복사 및 편집] 하여 사용한다.\n",
    "## 2. 아래의 [Empty Module 3개]를 직접 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compact-laundry",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:30.366193Z",
     "iopub.status.busy": "2021-06-07T02:08:30.364998Z",
     "iopub.status.idle": "2021-06-07T02:08:30.382056Z",
     "shell.execute_reply": "2021-06-07T02:08:30.382804Z",
     "shell.execute_reply.started": "2021-06-06T19:25:44.684637Z"
    },
    "papermill": {
     "duration": 0.047535,
     "end_time": "2021-06-07T02:08:30.383324",
     "exception": false,
     "start_time": "2021-06-07T02:08:30.335789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2021-ml-tp2-spam/train.csv\n",
      "/kaggle/input/2021-ml-tp2-spam/test.csv\n",
      "/kaggle/input/2021-ml-tp2-spam/sample_submit.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "talented-decade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:30.427167Z",
     "iopub.status.busy": "2021-06-07T02:08:30.426423Z",
     "iopub.status.idle": "2021-06-07T02:08:32.188972Z",
     "shell.execute_reply": "2021-06-07T02:08:32.188247Z",
     "shell.execute_reply.started": "2021-06-06T19:25:44.697418Z"
    },
    "papermill": {
     "duration": 1.784465,
     "end_time": "2021-06-07T02:08:32.189135",
     "exception": false,
     "start_time": "2021-06-07T02:08:30.404670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-march",
   "metadata": {
    "papermill": {
     "duration": 0.017442,
     "end_time": "2021-06-07T02:08:32.224669",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.207227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- nltk 라이브러리에서 punkt 데이터를 다운 받음, 이 데이터는 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer를 위해서 필요하다\n",
    "- nltk 라이브러리를 이용해서 불용어를 다운 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "received-millennium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:32.274799Z",
     "iopub.status.busy": "2021-06-07T02:08:32.274047Z",
     "iopub.status.idle": "2021-06-07T02:08:32.482131Z",
     "shell.execute_reply": "2021-06-07T02:08:32.482832Z",
     "shell.execute_reply.started": "2021-06-06T19:25:46.179806Z"
    },
    "papermill": {
     "duration": 0.239547,
     "end_time": "2021-06-07T02:08:32.483083",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.243536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unknown-sellers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:32.525983Z",
     "iopub.status.busy": "2021-06-07T02:08:32.525255Z",
     "iopub.status.idle": "2021-06-07T02:08:32.574593Z",
     "shell.execute_reply": "2021-06-07T02:08:32.574075Z",
     "shell.execute_reply.started": "2021-06-06T19:25:46.459269Z"
    },
    "papermill": {
     "duration": 0.072331,
     "end_time": "2021-06-07T02:08:32.574741",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.502410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       No I'm in the same boat. Still here at my moms...\n",
       "1       (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "2          They r giving a second chance to rahul dengra.\n",
       "3          O i played smash bros  &lt;#&gt;  religiously.\n",
       "4       PRIVATE! Your 2003 Account Statement for 07973...\n",
       "                              ...                        \n",
       "4452    I came hostel. I m going to sleep. Plz call me...\n",
       "4453                               Sorry, I'll call later\n",
       "4454        Prabha..i'm soryda..realy..frm heart i'm sory\n",
       "4455                           Nt joking seriously i told\n",
       "4456                  In work now. Going have in few min.\n",
       "Name: Data, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/train.csv\",encoding=\"latin-1\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/2021-ml-tp2-spam/test.csv\",encoding=\"latin-1\")\n",
    "df_train.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "formed-universe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:32.620287Z",
     "iopub.status.busy": "2021-06-07T02:08:32.619245Z",
     "iopub.status.idle": "2021-06-07T02:08:32.623802Z",
     "shell.execute_reply": "2021-06-07T02:08:32.623151Z",
     "shell.execute_reply.started": "2021-06-06T19:25:46.507079Z"
    },
    "papermill": {
     "duration": 0.030088,
     "end_time": "2021-06-07T02:08:32.623960",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.593872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    No I'm in the same boat. Still here at my moms...\n",
       "1    (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "2       They r giving a second chance to rahul dengra.\n",
       "3       O i played smash bros  &lt;#&gt;  religiously.\n",
       "4    PRIVATE! Your 2003 Account Statement for 07973...\n",
       "Name: Data, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_train[\"Data\"]\n",
    "y_train=df_train[\"Class\"]\n",
    "X_test=df_test[\"Data\"]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-event",
   "metadata": {
    "papermill": {
     "duration": 0.018746,
     "end_time": "2021-06-07T02:08:32.661768",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.643022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #1] 텍스트 데이터 전처리  \n",
    "\n",
    "목표: 텍스트 데이터를 처리하기 위한 여러 과정을 거쳐, 머신을 위한 데이터를 만든다. \n",
    "\n",
    "```\n",
    "[input]\n",
    "--------------\n",
    "- text: 텍스트 문장 데이터 \n",
    "\n",
    "[output]\n",
    "--------------\n",
    "- text: 전처리를 완료한 문장 데이터 \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "herbal-tongue",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:32.710466Z",
     "iopub.status.busy": "2021-06-07T02:08:32.709621Z",
     "iopub.status.idle": "2021-06-07T02:08:32.712286Z",
     "shell.execute_reply": "2021-06-07T02:08:32.712780Z",
     "shell.execute_reply.started": "2021-06-06T19:27:20.611981Z"
    },
    "papermill": {
     "duration": 0.032358,
     "end_time": "2021-06-07T02:08:32.712973",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.680615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #1] 텍스트 데이터 전처리\n",
    "# ------------------------------------------------\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def data_processing(text):\n",
    "    # ------------------------------------------------------------\n",
    "    # 구현 가이드라인 \n",
    "    # ------------------------------------------------------------\n",
    "    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n",
    "    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n",
    "    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n",
    "    #\n",
    "    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n",
    "    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n",
    "    #\n",
    "    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n",
    "    #\n",
    "    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n",
    "    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n",
    "    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n",
    "    #\n",
    "    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n",
    "    #\n",
    "    # [6] PorterStemmer 속 stem 내장 함수를 이용해 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n",
    "    #    \n",
    "    # ------------------------------------------------------------\n",
    "    ##############\n",
    "\n",
    "    # [1] A-Z, a-z를 제외한 문자는 공백으로 만들어 pre_words에 저장\n",
    "    pre_words = re.sub('[^A-Za-z]',' ', text)\n",
    "\n",
    "    # [2] pre_words의 문자들을 소문자로 변경\n",
    "    pre_words = pre_words.lower()\n",
    "\n",
    "    # [3] pre_words의 문장 토큰화해 tokenized_words에 저장\n",
    "    tokenized_words = word_tokenize(pre_words)\n",
    "\n",
    "    # [4] stops에 불용어 set 저장\n",
    "    stops =set(stopwords.words('english')) \n",
    "    \n",
    "    tokenized_words_remove=[]   \n",
    "    for w in tokenized_words: \n",
    "        # [5] tokenized_words의 단어가 불용어가 아니라면\n",
    "        if w not in stops: \n",
    "            # tokenized_words_remove에 해당 단어 추가\n",
    "            tokenized_words_remove.append(w)\n",
    "\n",
    "\n",
    "    stemmer = PorterStemmer()    # 어간추출\n",
    "    for i in range(len(tokenized_words_remove)):\n",
    "        # [6] stem 내장 함수를 이용해 어간 추출\n",
    "        tokenized_words_remove[i] = stemmer.stem(tokenized_words_remove[i])\n",
    "\n",
    "\n",
    "    return ( \" \".join( tokenized_words_remove ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awful-weapon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:32.758241Z",
     "iopub.status.busy": "2021-06-07T02:08:32.757588Z",
     "iopub.status.idle": "2021-06-07T02:08:36.350581Z",
     "shell.execute_reply": "2021-06-07T02:08:36.350032Z",
     "shell.execute_reply.started": "2021-06-06T13:16:12.134168Z"
    },
    "papermill": {
     "duration": 3.618295,
     "end_time": "2021-06-07T02:08:36.350735",
     "exception": false,
     "start_time": "2021-06-07T02:08:32.732440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train=[data_processing(i) for i in X_train]\n",
    "X_test=[data_processing(i) for i in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-canadian",
   "metadata": {
    "papermill": {
     "duration": 0.018559,
     "end_time": "2021-06-07T02:08:36.388556",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.369997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #2] Bag of Word \n",
    "\n",
    "목표: 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하기로한다. \n",
    "\n",
    "- train 과 test 데이터 전부 type을 ('U')로 변경하여 Countvectorizer를 사용한다. \n",
    "- [설명 링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spoken-format",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.441085Z",
     "iopub.status.busy": "2021-06-07T02:08:36.439976Z",
     "iopub.status.idle": "2021-06-07T02:08:36.672788Z",
     "shell.execute_reply": "2021-06-07T02:08:36.672055Z",
     "shell.execute_reply.started": "2021-06-06T13:07:48.597953Z"
    },
    "papermill": {
     "duration": 0.264785,
     "end_time": "2021-06-07T02:08:36.672953",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.408168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ------------------------------------------------\n",
    "# # [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# # ------------------------------------------------\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # ------------------------------------------------------------\n",
    "# # 구현 가이드라인 \n",
    "# # ------------------------------------------------------------\n",
    "# # [1]  CountVectorizer를 정의 \n",
    "# #           1) max_features를 100으로 지정 \n",
    "# # [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "# #\n",
    "# # [3] CountVectorizer를 이용해 X_train은 학습 및 변환을 하고, X_test는 변환을 진행. \n",
    "# # ------------------------------------------------------------\n",
    "# ###########\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer 생성 \n",
    "vectorizer = CountVectorizer(max_features = 200)\n",
    "             \n",
    "# numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "X_train = np.asarray(X_train).astype(\"U\")\n",
    "X_test = np.asarray(X_test).astype(\"U\")\n",
    "\n",
    "# X_train은 학습 및 변환을 하고, X_test는 변환\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "labeled-austin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.715936Z",
     "iopub.status.busy": "2021-06-07T02:08:36.714954Z",
     "iopub.status.idle": "2021-06-07T02:08:36.718353Z",
     "shell.execute_reply": "2021-06-07T02:08:36.717701Z",
     "shell.execute_reply.started": "2021-06-06T13:01:06.796837Z"
    },
    "papermill": {
     "duration": 0.026338,
     "end_time": "2021-06-07T02:08:36.718502",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.692164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# #  HashingVectorizer 생성 \n",
    "# vectorizer = HashingVectorizer()\n",
    "# #  numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "# X_train = np.asarray(X_train).astype(\"U\")\n",
    "# X_test = np.asarray(X_test).astype(\"U\")\n",
    "# # X_train은 학습 및 변환을 하고, X_test는 변환\n",
    "# X_train_features = vectorizer.fit_transform(X_train)\n",
    "# X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artistic-lightweight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.762252Z",
     "iopub.status.busy": "2021-06-07T02:08:36.761104Z",
     "iopub.status.idle": "2021-06-07T02:08:36.764508Z",
     "shell.execute_reply": "2021-06-07T02:08:36.763871Z",
     "shell.execute_reply.started": "2021-06-06T13:16:16.70138Z"
    },
    "papermill": {
     "duration": 0.026987,
     "end_time": "2021-06-07T02:08:36.764647",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.737660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# # TfidfVectorizer 생성 \n",
    "# vectorizer = TfidfVectorizer(max_features = 200)\n",
    "# #  numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "# X_train = np.asarray(X_train).astype(\"U\")\n",
    "# X_test = np.asarray(X_test).astype(\"U\")\n",
    "# # X_train은 학습 및 변환을 하고, X_test는 변환\n",
    "# X_train_features = vectorizer.fit_transform(X_train)\n",
    "# X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "applicable-neighborhood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.815636Z",
     "iopub.status.busy": "2021-06-07T02:08:36.814155Z",
     "iopub.status.idle": "2021-06-07T02:08:36.819252Z",
     "shell.execute_reply": "2021-06-07T02:08:36.818699Z",
     "shell.execute_reply.started": "2021-06-06T13:16:17.506601Z"
    },
    "papermill": {
     "duration": 0.035395,
     "end_time": "2021-06-07T02:08:36.819413",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.784018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spam인 데이터는 1, ham인 데이터는 0 대입\n",
    "y_train[y_train==\"ham\"]=0\n",
    "y_train[y_train==\"spam\"]=1\n",
    "y_train=y_train.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "synthetic-difference",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.862995Z",
     "iopub.status.busy": "2021-06-07T02:08:36.861991Z",
     "iopub.status.idle": "2021-06-07T02:08:36.865747Z",
     "shell.execute_reply": "2021-06-07T02:08:36.865130Z",
     "shell.execute_reply.started": "2021-06-06T13:16:18.945369Z"
    },
    "papermill": {
     "duration": 0.027427,
     "end_time": "2021-06-07T02:08:36.865883",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.838456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X_train_features\n",
    "y = y_train\n",
    "test = X_test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-values",
   "metadata": {
    "papermill": {
     "duration": 0.018672,
     "end_time": "2021-06-07T02:08:36.903777",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.885105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [Empty Module #3] SVM: classifier\n",
    "목표: SVC() 를 활용해 classification 을 진행\n",
    "\n",
    "- fit()으로 train 에 대한 머신러닝 학습\n",
    "- predict()으로 test 에 대한 정답을 추론 하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tender-inclusion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:36.947779Z",
     "iopub.status.busy": "2021-06-07T02:08:36.946880Z",
     "iopub.status.idle": "2021-06-07T02:08:36.961485Z",
     "shell.execute_reply": "2021-06-07T02:08:36.960854Z",
     "shell.execute_reply.started": "2021-06-06T13:16:21.800996Z"
    },
    "papermill": {
     "duration": 0.038671,
     "end_time": "2021-06-07T02:08:36.961638",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.922967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 성능 확인을 위한 train-validation set 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "studied-broadcasting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:37.008175Z",
     "iopub.status.busy": "2021-06-07T02:08:37.007377Z",
     "iopub.status.idle": "2021-06-07T02:08:45.828467Z",
     "shell.execute_reply": "2021-06-07T02:08:45.826980Z",
     "shell.execute_reply.started": "2021-06-06T13:16:23.333563Z"
    },
    "papermill": {
     "duration": 8.848028,
     "end_time": "2021-06-07T02:08:45.828729",
     "exception": false,
     "start_time": "2021-06-07T02:08:36.980701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  SVC 정의 \n",
    "#           1) gamma=\"auto\" 사용 \n",
    "# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "# 최적의 성능을 찾기 위해 GridSearch \n",
    "params = {\n",
    "    'C' : [3, 4, 5, 6, 7],\n",
    "    'kernel' : ['rbf'],\n",
    "    \n",
    "}\n",
    "clf = GridSearchCV(SVC(random_state = 0), params, cv=5)\n",
    "clf.fit(X, y)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contained-controversy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:45.874948Z",
     "iopub.status.busy": "2021-06-07T02:08:45.874209Z",
     "iopub.status.idle": "2021-06-07T02:08:46.265388Z",
     "shell.execute_reply": "2021-06-07T02:08:46.264624Z",
     "shell.execute_reply.started": "2021-06-06T13:18:31.316758Z"
    },
    "papermill": {
     "duration": 0.416798,
     "end_time": "2021-06-07T02:08:46.265594",
     "exception": false,
     "start_time": "2021-06-07T02:08:45.848796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775784753363229\n",
      "[[1153    6]\n",
      " [  24  155]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel = 'rbf', C = 5, random_state=0)\n",
    "clf.fit(X_train, ytrain)\n",
    "print(clf.score(X_test, ytest))\n",
    "\n",
    "y_p = clf.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.confusion_matrix(ytest, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-lebanon",
   "metadata": {
    "papermill": {
     "duration": 0.019367,
     "end_time": "2021-06-07T02:08:46.305230",
     "exception": false,
     "start_time": "2021-06-07T02:08:46.285863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regional-importance",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T02:08:46.351805Z",
     "iopub.status.busy": "2021-06-07T02:08:46.351143Z",
     "iopub.status.idle": "2021-06-07T02:08:46.853879Z",
     "shell.execute_reply": "2021-06-07T02:08:46.853314Z",
     "shell.execute_reply.started": "2021-06-06T13:17:17.777941Z"
    },
    "papermill": {
     "duration": 0.529286,
     "end_time": "2021-06-07T02:08:46.854062",
     "exception": false,
     "start_time": "2021-06-07T02:08:46.324776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 예측\n",
    "clf = SVC(kernel = 'rbf', C = 5, random_state=0)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(test)\n",
    "df_pred={\"ID\": range(np.array(y_pred).shape[0]),\"Class\":y_pred}\n",
    "df_pred=pd.DataFrame(df_pred)\n",
    "df_pred.to_csv(\"baseline_TF.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-motivation",
   "metadata": {
    "papermill": {
     "duration": 0.019515,
     "end_time": "2021-06-07T02:08:46.893684",
     "exception": false,
     "start_time": "2021-06-07T02:08:46.874169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.905501,
   "end_time": "2021-06-07T02:08:48.620515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-07T02:08:21.715014",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
